{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient test - compare derivatives of neuron output with internal gradient measurements of a model built with the NeuralLayer class and the Model class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Neural Layer Class\n",
    "\n",
    "<p> See <a href=\"twoLayerXOR.pdf\">documentation</a> for a discussion on the derivation of the classes used in this notebook. </p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pycodestyle_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%flake8_on --max_line_length 119"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class NeuralLayer:\n",
    "    '''\n",
    "    Class for defining a layer of nuerons that can have multiple inputs and neurons/outputs - a neuron can only\n",
    "    have one output, but it may server as inputs to many neurons in the next layer.  The equations used in this\n",
    "    class assume that the bias term is included in the weights vector and that the input to the weight is 1.0.\n",
    "    The equations assume that the loss function is the sum of the squares of the error where error is defined as\n",
    "    the difference between a known target value and the output of the Psi function.  The Psi function is\n",
    "    1/(1+e^-z).  And z, also known as net, is the sum of product of the weights and inputs (which includes the bias).\n",
    "    '''\n",
    "    def __init__(self, numberOfInputs, numberOfOutputs, learningFactor, id=\"me\", debug=False, filePath=None):\n",
    "        '''\n",
    "        Nueron constructor - uses tensors - last weight is the bias term\n",
    "        '''\n",
    "        self.id = id\n",
    "        self.debug = debug\n",
    "        self.numberOfInputs = numberOfInputs\n",
    "        self.numberOfNeurons = numberOfOutputs\n",
    "        self.backPropagatedErrorNotSet = True\n",
    "        self.learningFactor = learningFactor\n",
    "        self.normalizer = 2.0\n",
    "        self.delta = [0.0] * self.numberOfNeurons\n",
    "        self.weights = tf.random.uniform([numberOfInputs+1, numberOfOutputs], minval=-0.5, maxval=0.5,\n",
    "                                         dtype=tf.dtypes.float32)\n",
    "        self.error = [0.0] * numberOfOutputs\n",
    "        self.filePath = filePath\n",
    "        if filePath is not None:\n",
    "            try:\n",
    "                fileHandle = open(filePath, \"rb\")\n",
    "                self.weights = pickle.load(fileHandle)\n",
    "                fileHandle.close()\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "\n",
    "    def storeLayer(self, filePath):\n",
    "        '''\n",
    "        Store the weights that have been trained\n",
    "        '''\n",
    "        fileHandle = open(filePath, \"wb\")\n",
    "        pickle.dump(self.weights, fileHandle)\n",
    "        fileHandle.close()\n",
    "\n",
    "    def calculateOutput(self, inputs):\n",
    "        '''\n",
    "        Given the inputs, calculate the outputs\n",
    "        '''\n",
    "        self.inputs = tf.concat([inputs, [1.0]], 0)\n",
    "        self.outputs = self.psi(self.netAKAz())\n",
    "        return self.outputs\n",
    "\n",
    "    def netAKAz(self):\n",
    "        '''\n",
    "        Calculate the sum of the product of the weights and the inputs and add to the bia - this is net AKA z\n",
    "        '''\n",
    "        return tf.tensordot(self.inputs, self.weights, 1)\n",
    "\n",
    "    def psi(self, z):\n",
    "        '''\n",
    "        Apply the logistic function, ψ, to the outputs\n",
    "        '''\n",
    "        return 1.0 / (1.0 + tf.exp(-z))\n",
    "\n",
    "    def netWRTWeight(self, index):\n",
    "        '''\n",
    "        ∂zᵢ/∂wᵢ = inputᵢ  -- the change in neuron output with respect to a weight\n",
    "        '''\n",
    "        return self.inputs[index]\n",
    "\n",
    "    def netWRTWeightVector(self):\n",
    "        '''\n",
    "        ∂zᵢ/∂wᵢ = inputᵢ  -- the change in neuron output with respect to a weight - this is a vector\n",
    "        '''\n",
    "        return self.inputs\n",
    "\n",
    "    def psiWRTz(self, index):\n",
    "        '''\n",
    "        ∂ψᵢ/∂zᵢ = ψᵢ*(1-ψᵢ) where ψ = 1 / (1 + e^(-z)) -- the partial change of ψ with respect to z - this\n",
    "        is a scalar - must designate output index\n",
    "        '''\n",
    "        return self.outputs[index]*(1 - self.outputs[index])\n",
    "\n",
    "    def errorWRTPsi(self, targetArray, index):\n",
    "        '''\n",
    "        ∂Eᵢ/∂ψᵢ =  -(targetOutput - ψᵢ)  # assuming that E is square of the error and ignoring the gain (2) -\n",
    "        this is a scalar must designate output index\n",
    "        '''\n",
    "        if (self.backPropagatedErrorNotSet):\n",
    "            targetOutput = targetArray[index]\n",
    "            self.error[index] = - (self.normalizer * (targetOutput - self.outputs[index]))\n",
    "        else:\n",
    "            pass  # should have been set by a higher layer\n",
    "        return self.error[index]\n",
    "\n",
    "    def updateWeights(self, target=None, deltas=None):\n",
    "        '''\n",
    "        Update the weights to minimize the loss - if in batch mode, the deltas have been accumulated by updateDeltas\n",
    "        '''\n",
    "        if deltas is None:\n",
    "            deltas = self.updateDeltas(target)\n",
    "        self.weights -= self.learningFactor * tf.transpose(deltas)\n",
    "\n",
    "    def updateDeltas(self, target, deltas=None):\n",
    "        '''\n",
    "        Update the deltas during batch processing\n",
    "        '''\n",
    "        for neuron in range(self.numberOfNeurons):\n",
    "            if neuron == 0:\n",
    "                deltaDeltas = tf.reshape(tf.convert_to_tensor(self.errorWRTPsi(target, neuron)\n",
    "                                                              * self.psiWRTz(neuron)\n",
    "                                                              * self.netWRTWeightVector()),\n",
    "                                         [1, len(self.netWRTWeightVector())])  # make a 1 by n vector\n",
    "            else:\n",
    "                deltaDeltas = tf.concat((deltaDeltas, [self.errorWRTPsi(target, neuron)\n",
    "                                                       * self.psiWRTz(neuron)\n",
    "                                                       * self.netWRTWeightVector()]), 0)  # tack on a new row\n",
    "            if self.debug:\n",
    "                print(\"updateDeltas - layer {}, neuron {}, weight deltaDeltas\\n{}\".\n",
    "                      format(self.id, neuron, deltaDeltas))\n",
    "\n",
    "        if deltas is None:\n",
    "            deltas = deltaDeltas\n",
    "        else:\n",
    "            deltas += deltaDeltas\n",
    "        self.propagateError()  # do this before updating weights\n",
    "        return deltas\n",
    "\n",
    "    def propagateError(self):\n",
    "        '''\n",
    "        Determine error to send to previous layers\n",
    "        For each neuron, determine the amount of error at it's output that needs to be applied to the input\n",
    "        which is the output of the previous level.  Those individual neuron amounts then need to be summed\n",
    "        across all neurons.\n",
    "        '''\n",
    "        previousLayerNeuronError = [0.0] * (self.numberOfInputs + 1)\n",
    "        for thisLayerNeuron in range(self.numberOfNeurons):\n",
    "            error = self.error[thisLayerNeuron]\n",
    "            amountForEachPreviousLayerNeuron = error * self.weights[:, thisLayerNeuron] * self.psiWRTz(thisLayerNeuron)\n",
    "            if self.debug:\n",
    "                print(\"sum of weights for neurons at this layer: {}\".\n",
    "                      format(tf.reduce_sum(self.weights[:, thisLayerNeuron])))\n",
    "                print(\"propagateError - in layer {}, neuron {}, contribution:{}\".\n",
    "                      format(self.id, thisLayerNeuron, amountForEachPreviousLayerNeuron))\n",
    "                print(\"propagateError - Error {}, weights {}\".format(error, self.weights[:, thisLayerNeuron]))\n",
    "            previousLayerNeuronError += amountForEachPreviousLayerNeuron\n",
    "        self.errorForNextLayer = previousLayerNeuronError\n",
    "        if self.debug:\n",
    "            print(\"propagateError - in layer {}, the next layer's error will be\\n {}\".\n",
    "                  format(self.id, previousLayerNeuronError))\n",
    "\n",
    "    def setPropagationError(self, error):\n",
    "        '''\n",
    "        From a higher layer, set the error propogating back to this layer\n",
    "        '''\n",
    "        self.error = error\n",
    "        if self.debug:\n",
    "            print(\"setPropagationError - setting propagation error in layer {} to\\n {}\".\n",
    "                  format(self.id, self.error))\n",
    "        self.backPropagatedErrorNotSet = False\n",
    "\n",
    "    def setLearningFactor(self, factor):\n",
    "        '''\n",
    "        Setter for learning factor\n",
    "        '''\n",
    "        self.learningFactor = factor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Model Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class Model:\n",
    "    '''\n",
    "    Class for defining a model that consists of neural layers.  The first layer is always the inputs, which\n",
    "    means that it does not exist as a NeuralLayer.  All subsequent layers are completely interconnected\n",
    "    except for the final layer.\n",
    "    '''\n",
    "    def __init__(self, inputOutputList, debug=False, filePath=None):\n",
    "        '''\n",
    "        Model constructor - contrusts layers from the list entries\n",
    "        '''\n",
    "        self.layers = []\n",
    "        layerIndex = 0\n",
    "\n",
    "        for entryTuple in inputOutputList:\n",
    "            layerid = None\n",
    "            if isinstance(entryTuple[-1], str):\n",
    "                layerid = entryTuple[-1]\n",
    "            inputs = entryTuple[0]\n",
    "            outputs = entryTuple[1]\n",
    "            learningFactor = entryTuple[2]\n",
    "            if filePath is None:\n",
    "                weightFilePath = filePath\n",
    "            else:\n",
    "                weightFilePath = filePath + str(layerIndex)\n",
    "                layerIndex += 1\n",
    "            self.layers.append(NeuralLayer(inputs, outputs, learningFactor, layerid, debug, weightFilePath))\n",
    "\n",
    "    def storeModel(self, filePath):\n",
    "        '''\n",
    "        Store the weights for all of the layers of this model\n",
    "        '''\n",
    "        layerIndex = 0\n",
    "        for layer in self.layers:\n",
    "            layer.storeLayer(filePath + str(layerIndex))\n",
    "            layerIndex += 1\n",
    "\n",
    "    def feedForward(self, inputs):\n",
    "        '''\n",
    "        Given the inputs, propagate them through the model layers\n",
    "        '''\n",
    "        layerOutputs = inputs\n",
    "        for aLayer in self.layers:\n",
    "            layerOutputs = aLayer.calculateOutput(layerOutputs)\n",
    "        return layerOutputs\n",
    "\n",
    "    def updateDeltas(self, target, deltas=None):\n",
    "        '''\n",
    "        Update the deltas in all the layers\n",
    "        '''\n",
    "        reversedLayers = self.layers.copy()\n",
    "        reversedLayers.reverse()\n",
    "        lastLayer = len(reversedLayers) - 1\n",
    "        newDeltaList = False\n",
    "        if deltas is None:\n",
    "            deltas = []  # make a list of deltas, one for each layer\n",
    "            newDeltaList = True\n",
    "        for index, layer in enumerate(reversedLayers):\n",
    "            if newDeltaList:\n",
    "                deltas.append(layer.updateDeltas(target))\n",
    "            else:\n",
    "                deltas[index] = layer.updateDeltas(target, deltas=deltas[index])\n",
    "            if index < lastLayer:\n",
    "                reversedLayers[index+1].setPropagationError(layer.errorForNextLayer)\n",
    "        return deltas\n",
    "\n",
    "    def updateWeights(self, target=None, deltas=None):\n",
    "        '''\n",
    "        Update the weights and propagate the error of all layers\n",
    "        '''\n",
    "        reversedLayers = self.layers.copy()\n",
    "        reversedLayers.reverse()\n",
    "        lastLayer = len(reversedLayers) - 1\n",
    "        for index, layer in enumerate(reversedLayers):\n",
    "            if deltas is None:\n",
    "                layer.updateWeights(target)\n",
    "                if index < lastLayer:\n",
    "                    reversedLayers[index+1].setPropagationError(layer.errorForNextLayer)\n",
    "            else:\n",
    "                layer.updateWeights(deltas=deltas[index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient test - approach: set initial conditions, perturb a weight, measure its affect, compare it to the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++\n",
      "layer 0, neuron 0, weight 0\n",
      "layer 0, input: [0.5 0.5 1. ]\n",
      "First Output [0.56585276], Second Output [0.5658558]\n",
      "Model Gradient: 0.003440552158281207, Calculated Gradient [0.00342727], ratio [1.0038763]\n",
      "++++++++\n",
      "++++++++\n",
      "layer 0, neuron 0, weight 1\n",
      "layer 0, input: [0.5 0.5 1. ]\n",
      "First Output [0.56585276], Second Output [0.5658558]\n",
      "Model Gradient: 0.003440552158281207, Calculated Gradient [0.00342727], ratio [1.0038763]\n",
      "++++++++\n",
      "++++++++\n",
      "layer 0, neuron 0, weight 2\n",
      "layer 0, input: [0.5 0.5 1. ]\n",
      "First Output [0.56585276], Second Output [0.56585884]\n",
      "Model Gradient: 0.006880785804241896, Calculated Gradient [0.00688434], ratio [0.9994843]\n",
      "++++++++\n",
      "++++++++\n",
      "layer 0, neuron 1, weight 0\n",
      "layer 0, input: [0.5 0.5 1. ]\n",
      "First Output [0.56585276], Second Output [0.5658558]\n",
      "Model Gradient: 0.003440552158281207, Calculated Gradient [0.00342727], ratio [1.0038763]\n",
      "++++++++\n",
      "++++++++\n",
      "layer 0, neuron 1, weight 1\n",
      "layer 0, input: [0.5 0.5 1. ]\n",
      "First Output [0.56585276], Second Output [0.5658558]\n",
      "Model Gradient: 0.003440552158281207, Calculated Gradient [0.00342727], ratio [1.0038763]\n",
      "++++++++\n",
      "++++++++\n",
      "layer 0, neuron 1, weight 2\n",
      "layer 0, input: [0.5 0.5 1. ]\n",
      "First Output [0.56585276], Second Output [0.56585884]\n",
      "Model Gradient: 0.006880785804241896, Calculated Gradient [0.00688434], ratio [0.9994843]\n",
      "++++++++\n",
      "++++++++\n",
      "layer 0, neuron 2, weight 0\n",
      "layer 0, input: [0.5 0.5 1. ]\n",
      "First Output [0.56585276], Second Output [0.5658558]\n",
      "Model Gradient: 0.003440552158281207, Calculated Gradient [0.00342727], ratio [1.0038763]\n",
      "++++++++\n",
      "++++++++\n",
      "layer 0, neuron 2, weight 1\n",
      "layer 0, input: [0.5 0.5 1. ]\n",
      "First Output [0.56585276], Second Output [0.5658558]\n",
      "Model Gradient: 0.003440552158281207, Calculated Gradient [0.00342727], ratio [1.0038763]\n",
      "++++++++\n",
      "++++++++\n",
      "layer 0, neuron 2, weight 2\n",
      "layer 0, input: [0.5 0.5 1. ]\n",
      "First Output [0.56585276], Second Output [0.56585884]\n",
      "Model Gradient: 0.006880785804241896, Calculated Gradient [0.00688434], ratio [0.9994843]\n",
      "++++++++\n",
      "++++++++\n",
      "layer 1, neuron 0, weight 0\n",
      "layer 1, input: [0.54983395 0.54983395 0.54983395 1.        ]\n",
      "First Output [0.56585276], Second Output [0.5659879]\n",
      "Model Gradient: 0.15288950502872467, Calculated Gradient [0.15291572], ratio [0.9998286]\n",
      "++++++++\n",
      "++++++++\n",
      "layer 1, neuron 0, weight 1\n",
      "layer 1, input: [0.54983395 0.54983395 0.54983395 1.        ]\n",
      "First Output [0.56585276], Second Output [0.5659879]\n",
      "Model Gradient: 0.15288950502872467, Calculated Gradient [0.15291572], ratio [0.9998286]\n",
      "++++++++\n",
      "++++++++\n",
      "layer 1, neuron 0, weight 2\n",
      "layer 1, input: [0.54983395 0.54983395 0.54983395 1.        ]\n",
      "First Output [0.56585276], Second Output [0.5659879]\n",
      "Model Gradient: 0.15288950502872467, Calculated Gradient [0.15291572], ratio [0.9998286]\n",
      "++++++++\n",
      "++++++++\n",
      "layer 1, neuron 0, weight 3\n",
      "layer 1, input: [0.54983395 0.54983395 0.54983395 1.        ]\n",
      "First Output [0.56585276], Second Output [0.5660984]\n",
      "Model Gradient: 0.2781026363372803, Calculated Gradient [0.27802587], ratio [1.0002761]\n",
      "++++++++\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def printBasicInfo(one, two, layer, neuron, weight, deltas, model):\n",
    "    '''\n",
    "    print summary gradient information\n",
    "    '''\n",
    "    top = len(model.layers) - 1\n",
    "    print(\"++++++++\")\n",
    "    print(\"layer {}, neuron {}, weight {}\".format(layer, neuron, weight))\n",
    "    print(\"layer {}, input: {}\".format(layer, model.layers[layer].inputs))\n",
    "    print(\"First Output {}, Second Output {}\".format(one, two))\n",
    "    calcG = (two*two - one*one)/0.001\n",
    "    ratio = \"--\"\n",
    "    if calcG != 0.0:\n",
    "        ratio = deltas[top-layer][neuron][weight]/calcG\n",
    "    print(\"Model Gradient: {}, Calculated Gradient {}, ratio {}\".\n",
    "          format(deltas[top-layer][neuron][weight], calcG, ratio))\n",
    "    print(\"++++++++\")\n",
    "\n",
    "\n",
    "def getLocation():\n",
    "    '''\n",
    "    get the line number of the print\n",
    "    '''\n",
    "    callerFrameRecord = inspect.stack()[1]\n",
    "    frame = callerFrameRecord[0]\n",
    "    info = inspect.getframeinfo(frame)\n",
    "    returnString = \"trace from: \" + str(info.function) + \", line: \" + str(info.lineno) + \" ----> \"\n",
    "    return returnString\n",
    "\n",
    "\n",
    "def setWeight(weights, layer, neuron, weight, value, model):\n",
    "    '''\n",
    "    set one of the weights to a slightly different value\n",
    "    '''\n",
    "    rowsCols = tf.shape(weights[layer])\n",
    "    # the row is the weight, the column is the neuron\n",
    "    vector = [0.1] * (rowsCols[1] * rowsCols[0]).numpy()\n",
    "    vector[weight * rowsCols[1].numpy() + neuron] = value\n",
    "    weights[layer] = tf.reshape(vector, rowsCols)\n",
    "    model.layers[layer].weights = weights[layer]\n",
    "    return weights\n",
    "\n",
    "\n",
    "def generalizedGradientSweep(modelDefinition, debug=False):\n",
    "    '''\n",
    "    Using the model definition, find the gradient at every weight within the model\n",
    "    '''\n",
    "    numberOfLayers = len(modelDefinition)\n",
    "    model = Model(modelDefinition, debug)\n",
    "    weightShape = []\n",
    "    for layer in range(numberOfLayers):\n",
    "        weightShape.append(tf.shape(model.layers[layer].weights))\n",
    "        if debug:\n",
    "            print(\"{} weight shape at layer {} is {}\".format(getLocation(), layer, weightShape[-1]))\n",
    "    weightsForLayers = []\n",
    "    for layer in range(numberOfLayers):\n",
    "        weightsThisLayer = weightShape[layer][0] * weightShape[layer][1]\n",
    "        vectorOfWeights = [0.1] * weightsThisLayer.numpy()\n",
    "        vectorOfWeights = tf.convert_to_tensor(vectorOfWeights)\n",
    "        matrixOfWeights = tf.reshape(vectorOfWeights, weightShape[layer])\n",
    "        weightsForLayers.append(matrixOfWeights)\n",
    "    if debug:\n",
    "        print(\"{} initial weight settings: {}\".format(getLocation(), weightsForLayers))\n",
    "    for layer in range(numberOfLayers):\n",
    "        model.layers[layer].weights = weightsForLayers[layer]\n",
    "        if debug:\n",
    "            print(\"{} model weights for layer {}:\\n{}\".format(getLocation(), layer, model.layers[layer].weights))\n",
    "    vectorOfInput = [0.5] * (tf.shape(weightsForLayers[0])[0].numpy() - 1)\n",
    "    if debug:\n",
    "        print(\"{} vectorOfInput: {}, type: {}\".format(getLocation(), vectorOfInput, type(vectorOfInput)))\n",
    "    output0 = model.feedForward(vectorOfInput)\n",
    "    for layer in range(numberOfLayers):\n",
    "        numberOfNeurons = weightShape[layer][1]\n",
    "        numberOfWeights = weightShape[layer][0]\n",
    "        if debug:\n",
    "            print(\"{} in layer: {} number of neurons: {}, number of weights: {}\".\n",
    "                  format(getLocation(), layer, numberOfNeurons, numberOfWeights))\n",
    "        for neuron in range(numberOfNeurons):\n",
    "            for weight in range(numberOfWeights):\n",
    "                weightsForLayers = setWeight(weightsForLayers, layer, neuron, weight, 0.101, model)\n",
    "                output1 = model.feedForward(vectorOfInput)\n",
    "                deltas = model.updateDeltas([0.0])\n",
    "                if debug:\n",
    "                    print(\"{} weightsForLayer\\n{}\".format(getLocation(), weightsForLayers))\n",
    "                    print(\"{} deltas\\n{}\".format(getLocation(), deltas))\n",
    "                printBasicInfo(output0, output1, layer, neuron, weight, deltas, model)\n",
    "                weightsForLayers = setWeight(weightsForLayers, layer, neuron, weight, 0.1, model)\n",
    "\n",
    "\n",
    "def main():\n",
    "    '''\n",
    "    Gradient test\n",
    "    '''\n",
    "    generalizedGradientSweep([(2, 3, 1000.0, \"0\"), (3, 1, 1000.0, \"1\")])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
